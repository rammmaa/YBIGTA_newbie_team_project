import os
import glob
import json
from datetime import datetime
import pandas as pd
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer

# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ êµ¬ì¡° ê³„ì‚°
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))  # st_app/
PROJECT_ROOT = os.path.dirname(BASE_DIR)  # repo root

# 1. ê²½ë¡œ ì„¤ì •
reviews_dir = os.path.join(PROJECT_ROOT, "database")               # CSV í´ë”
save_dir    = os.path.join(BASE_DIR, "db", "faiss_index")          # index ì €ì¥ í´ë”
index_path  = os.path.join(save_dir, "index.faiss")
meta_path   = os.path.join(save_dir, "meta.json")
docs_path   = os.path.join(save_dir, "docs.jsonl")                 #  ì¶”ê°€: ë¬¸ì„œ ë§¤í•‘

os.makedirs(save_dir, exist_ok=True)

# 2. ì „ì²˜ë¦¬ ë¦¬ë·° íŒŒì¼ ì½ê¸°
file_list = glob.glob(os.path.join(reviews_dir, "preprocessed_reviews_*.csv"))
df_list = []

for file in file_list:
    try:
        df = pd.read_csv(file)
        df_list.append(df)
        print(f"âœ… {file} ì½ê¸° ì™„ë£Œ (í–‰ {len(df)})")
    except Exception as e:
        print(f"âŒ {file} ì½ê¸° ì‹¤íŒ¨: {e}")

if not df_list:
    raise ValueError("preprocessed_reviews íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

all_reviews = pd.concat(df_list, ignore_index=True)

# 3. í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì§€ì • + ì „ì²˜ë¦¬(â˜… ì¤‘ìš”)
if "content" not in all_reviews.columns:
    raise ValueError("'content' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì‹¤ì œ ì»¬ëŸ¼ëª… í™•ì¸ í•„ìš”.")

# ê²°ì¸¡/ê³µë°± ì œê±°
all_reviews = all_reviews.dropna(subset=["content"])
all_reviews["content"] = all_reviews["content"].astype(str).str.strip()

# ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œê±°(ê¸¸ì´<5)
all_reviews = all_reviews[all_reviews["content"].str.len() >= 5]

# ì™„ì „ ì¤‘ë³µ ì œê±°
all_reviews = all_reviews.drop_duplicates(subset=["content"]).reset_index(drop=True)

# í•„ìš”í•˜ë‹¤ë©´ ë‚ ì§œ/ì‚¬ì´íŠ¸ ê¸°ì¤€ ì •ë ¬(ì¬í˜„ì„± â†‘) â€” ì»¬ëŸ¼ ì—†ìœ¼ë©´ ë¬´ì‹œ
sort_cols = [c for c in ["date", "site_name", "rating"] if c in all_reviews.columns]
if sort_cols:
    all_reviews = all_reviews.sort_values(sort_cols).reset_index(drop=True)

texts = all_reviews["content"].tolist()
print(f"ğŸ§¹ ì „ì²˜ë¦¬ í›„ ë¬¸ì„œ ìˆ˜: {len(texts)}")

# 4. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedding_model_name = "all-MiniLM-L6-v2"
model = SentenceTransformer(embedding_model_name)

# 5. ë¬¸ì¥ â†’ ë²¡í„° ë³€í™˜ (ë°°ì¹˜ ì¸ì½”ë”© ê¶Œì¥)
embeddings = model.encode(
    texts,
    batch_size=128,               # í•„ìš”ì‹œ ì¡°ì •
    show_progress_bar=True,
    convert_to_numpy=True,
    normalize_embeddings=False    # L2 ì¸ë±ìŠ¤ì´ë¯€ë¡œ ì •ê·œí™” X
).astype(np.float32)

# 6. FAISS ì¸ë±ìŠ¤ ìƒì„± (L2 ê±°ë¦¬)  â˜… retrieverë„ L2ë¡œ ë§ì¶œ ê²ƒ
d = embeddings.shape[1]
index = faiss.IndexFlatL2(d)

# 7. ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€
index.add(embeddings)
print(f"ğŸ“¦ ì¸ë±ìŠ¤ì— ì¶”ê°€ëœ ë²¡í„° ìˆ˜: {index.ntotal}")

# 8. ì¸ë±ìŠ¤ ì €ì¥
faiss.write_index(index, index_path)
print(f"ğŸ’¾ ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {index_path}")

# 9. ë¬¸ì„œ ë§¤í•‘ ì €ì¥(â˜… ì¤‘ìš”: ì¸ë±ìŠ¤ ìˆœì„œ == íŒŒì¼ ê¸°ë¡ ìˆœì„œ)
with open(docs_path, "w", encoding="utf-8") as f:
    for i, row in all_reviews.iterrows():
        rec = {
            "id": row.get("id", f"doc_{i}"),
            "content": row["content"],
            "site_name": row.get("site_name"),
            "rating": row.get("rating"),
            "date": row.get("date"),
            "subject_id": row.get("subject_id"),
            "source": row.get("source"),   # ìˆìœ¼ë©´ ê¸°ë¡
        }
        f.write(json.dumps(rec, ensure_ascii=False) + "\n")
print(f"ğŸ—ƒï¸ docs.jsonl ì €ì¥ ì™„ë£Œ: {docs_path}")

# 10. ë©”íƒ€ ì €ì¥(ì„ë² ë”©-ê²€ìƒ‰ ì¼ê´€ì„± ê¸°ë¡)
meta_data = {
    "embedding_model": embedding_model_name,
    "vector_dimension": d,
    "index_type": "IndexFlatL2",
    "metric": "L2",                     # retriever ì°¸ê³ ìš©
    "total_vectors": int(index.ntotal),
    "created_at": datetime.now().isoformat(),
    "source_files": [os.path.basename(f) for f in file_list],
    "docs_path": docs_path              # â˜… retrieverê°€ ì´ ê²½ë¡œë¥¼ ë¡œë“œ
}

with open(meta_path, "w", encoding="utf-8") as f:
    json.dump(meta_data, f, ensure_ascii=False, indent=4)

print(f"ğŸ“ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {meta_path}")
